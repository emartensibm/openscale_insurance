{
 "nbformat_minor": 1,
 "cells": [
  {
   "source": [
    "# Infuse Applications with AI Using IBM Watson OpenScale"
   ],
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   }
  },
  {
   "source": [
    "The following notebook is intended for use with the Watson OpenScale hands-on lab found [here](https://dtelink). It contains instructions and data for training and deploying an insurance fraud prediction model, and configuring Watson OpenScale to monitor and provide detailed explanations for that model's predictions.\n",
    "\n",
    "This notebook should be run in a Watson Studio project, using a Python 3.5 or above runtime environment. If you are viewing this in Watson Studio and do not see Python 3.5 or above in the upper right corner of your screen, please update the runtime now. It requires service credentials for the following Cloud services:\n",
    "\n",
    "* __IBM Watson OpenScale__\n",
    "* __Watson Machine Learning__\n",
    "\n",
    "If you have a paid Cloud account, you may also provision a __Databases for PostgreSQL__ or __Db2 Warehouse__ service to take full advantage of integration with Watson Studio and continuous learning services. If you choose not to provision this paid service, you can use the free internal PostgreSQL storage with OpenScale, but will not be able to configure continuous learning for your model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Install packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "!pip install --upgrade ibm-ai-openscale --no-cache | tail -n 1\n",
    "!pip install --upgrade watson-machine-learning-client | tail -n 1\n",
    "!pip install --upgrade numpy --no-cache | tail -n 1\n",
    "!pip install --upgrade SciPy --no-cache | tail -n 1\n",
    "!pip install lime --no-cache | tail -n 1\n",
    "!pip install 'scikit-learn==0.19.1' --force-reinstall"
   ],
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy.io import arff\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ],
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "## Provision services and configure credentials\n",
    "\n",
    "In this section, you will add your credentials for Watson Machine Learning and OpenScale. If you have not already, provision an instance of IBM Watson OpenScale using the [OpenScale link in the Cloud catalog](https://cloud.ibm.com/catalog/services/watson-openscale).\n",
    "\n",
    "Your Cloud API key can be generated by going to the [__Users__ section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the __API Keys__ section, and click __Create an IBM Cloud API key__. Give your key a name and click __Create__, then copy the created key and paste it between the single quotes in the cell below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "CLOUD_API_KEY = '__PASTE_HERE___'"
   ],
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "Next you will need credentials for Watson Machine Learning. If you already have a WML instance, you may use credentials for it. To provision a new Lite instance of WML, use the [Cloud catalog](https://cloud.ibm.com/catalog/services/machine-learning), give your service a name, and click __Create__. Once your instance is created, click the __Service Credentials__ link on the left side of the screen. Click the __New credential__ button, give your credentials a name, and click __Add__. Your new credentials can be accessed by clicking the __View credentials__ button. Copy and paste your WML credentials into the cell below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "WML_CREDENTIALS = {\n",
    "    \"apikey\": \"key\",\n",
    "    \"iam_apikey_description\": \"description\",\n",
    "    \"iam_apikey_name\": \"auto-generated-apikey\",\n",
    "    \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "    \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::\",\n",
    "    \"instance_id\": \"instance_id\",\n",
    "    \"password\": \"password\",\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"username\": \"username\"\n",
    "}"
   ],
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "This tutorial can use Databases for PostgreSQL, Db2 Warehouse, or a free internal version of PostgreSQL to create a datamart for OpenScale. The free internal version can be accessed via the OpenScale APIs, but you will be unable to access it using direct database queries.\n",
    "\n",
    "If you have previously configured OpenScale, it will use your existing datamart, and not interfere with any models you are currently monitoring. Do not update the cell below.\n",
    "\n",
    "If you do not have a paid Cloud account or would prefer not to provision this paid service, you may use the free internal PostgreSQL service with OpenScale. Do not update the cell below.\n",
    "\n",
    "To provision a new instance of Db2 Warehouse, locate [Db2 Warehouse in the Cloud catalog](https://cloud.ibm.com/catalog/services/db2-warehouse), give your service a name, and click __Create__. Once your instance is created, click the __Service Credentials__ link on the left side of the screen. Click the __New credential__ button, give your credentials a name, and click __Add__. Your new credentials can be accessed by clicking the __View credentials__ button. Copy and paste your Db2 Warehouse credentials into the cell below.\n",
    "\n",
    "To provision a new instance of Databases for PostgreSQL, locate [Databases for PostgreSQL](https://cloud.ibm.com/catalog/services/databases-for-postgresql) in the Cloud catalog, give your service a name, and click __Create__. Once your instance is created, click the __Service Credentials__ link on the left side of the screen. Click the __New credential__ button, give your credentials a name, and click __Add__. Your new credentials can be accessed by clicking the __View credentials__ button. Copy and paste your Databases for PostgreSQL credentials into the cell below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "DB_CREDENTIALS = None"
   ],
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": [
    "## Restart the kernel and run the notebook\n",
    "\n",
    "At this point, the notebook is ready to run. _You must restart the kernel via the kernel menu above_. You can either restart the kernel and run the cells one at a time, starting from the package installation, or click the __Kernel__ option above and select __Restart and Run All__ to run all the cells."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "### Get the training data from github",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "!rm training_data.csv\n!wget https://raw.githubusercontent.com/emartensibm/openscale_insurance/master/data/training_data.csv",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "### Explore the data\n\nThe training data contains information on auto insurance claims that may indicate a higher likelihood of fraudulent claims. In this case, we have a set of binary variables for the following:\n* __SUSPICIOUS\\_CLAIM\\_TIME__: The claim was filed after too much time had elapsed following the incident\n* __EXPIRED\\_LICENSE__: The person filing the claim did not have a valid drivers license at the time of the incident\n* __LOW\\_MILES\\_AT\\_LOSS__: The vehicle's mileage at the time of loss was lower than expected\n* __EXCESSIVE\\_CLAIM\\_AMOUNT__: The dollar amount claimed was higher than expected given the value of the vehicle\n* __TOO\\_MANY\\_CLAIMS__: The person filing the claim has multiple claims outstanding\n* __NO\\_POLICE__: No police report was filed for the loss incident",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "features = [\"SUSPICIOUS_CLAIM_TIME\", \"EXPIRED_LICENSE\", \"LOW_MILES_AT_LOSS\", \"EXCESSIVE_CLAIM_AMOUNT\", \"TOO_MANY_CLAIMS\", \"NO_POLICE\", \"FLAG_FOR_FRAUD_INV\"]\ndf_model = pd.read_csv('training_data.csv')\n\ndf_model.drop([\"DRIVER_ID\", \"POLICY_ID\", \"CLAIM_ID\", \"HOUSEHOLD_ID\", \"ZIPCODE\"], axis=1, inplace=True)\n\ndf_model[\"SUSPICIOUS_CLAIM_TIME\"] = df_model[\"SUSPICIOUS_CLAIM_TIME\"].astype(int)\ndf_model[\"EXPIRED_LICENSE\"] = df_model[\"EXPIRED_LICENSE\"].astype(int)\ndf_model[\"LOW_MILES_AT_LOSS\"] = df_model[\"LOW_MILES_AT_LOSS\"].astype(int)\ndf_model[\"EXCESSIVE_CLAIM_AMOUNT\"] = df_model[\"EXCESSIVE_CLAIM_AMOUNT\"].astype(int)\ndf_model[\"TOO_MANY_CLAIMS\"] = df_model[\"TOO_MANY_CLAIMS\"].astype(int)\ndf_model[\"NO_POLICE\"] = df_model[\"NO_POLICE\"].astype(int)\ndf_model[\"FLAG_FOR_FRAUD_INV\"] = df_model[\"FLAG_FOR_FRAUD_INV\"].astype(int)\n\ndf_model.head()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Identify the training data columns and label columns, and set up a train/test split of 80/20.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "xVar = df_model[[\"SUSPICIOUS_CLAIM_TIME\", \"EXPIRED_LICENSE\", \"LOW_MILES_AT_LOSS\", \"EXCESSIVE_CLAIM_AMOUNT\", \"TOO_MANY_CLAIMS\", \"NO_POLICE\"]]\nyVar = df_model[\"FLAG_FOR_FRAUD_INV\"]\n\nx_train, x_test, y_train, y_test = train_test_split(xVar, yVar, test_size=0.2)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Create a scikit-learn Random Forest Classifier and fit the training data.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "model = RandomForestClassifier(n_jobs=2, random_state=0)\nmodel.fit(x_train, y_train)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Check the test data using the model. For this model, an output of 1 indicates likely fraud; an output of 0 indicates unlikely fraud.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "predict_result = model.predict(x_test)\npd.crosstab(y_test, predict_result, rownames = [\"Actual Result\"], colnames = [\"Predicted Result\"])",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "## Store the model in Watson Machine Learning\n\nIn this section, the notebook uses the supplied Watson Machine Learning credentials to save the model to the WML instance. Previous versions of the model are removed so that the notebook can be run again, resetting all data for another demo.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "wml_client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "wml_client.repository.list_models()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "MODEL_NAME = \"SKLearn Fraud Prediction\"\nDEPLOYMENT_NAME = \"SKLearn Fraud Deployment\"",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "model_deployment_ids = wml_client.deployments.get_uids()\nfor deployment_id in model_deployment_ids:\n    deployment = wml_client.deployments.get_details(deployment_id)\n    model_id = deployment['entity']['deployable_asset']['guid']\n    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n        print('Deleting deployment id', deployment_id)\n        wml_client.deployments.delete(deployment_id)\n        print('Deleting model id', model_id)\n        wml_client.repository.delete(model_id)\nwml_client.repository.list_models()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "model_props = {\n    wml_client.repository.ModelMetaNames.NAME: \"{}\".format(MODEL_NAME),\n    wml_client.repository.ModelMetaNames.EVALUATION_METHOD: \"binary\",\n    wml_client.repository.ModelMetaNames.FRAMEWORK_NAME: \"scikit-learn\",\n    wml_client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"0.19\",\n    wml_client.repository.ModelMetaNames.RUNTIME_NAME: \"python\",\n    wml_client.repository.ModelMetaNames.RUNTIME_VERSION: \"3.5\"\n}\n\ndf_train = df_model.copy()\ndf_train.drop(\"FLAG_FOR_FRAUD_INV\", axis=1, inplace=True)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "df_train.head()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "wml_models = wml_client.repository.get_details()\nmodel_uid = None\nfor model_in in wml_models['models']['resources']:\n    if MODEL_NAME == model_in['entity']['name']:\n        model_uid = model_in['metadata']['guid']\n        break\n\nif model_uid is None:\n    print(\"Storing model ...\")\n\n    published_model_details = wml_client.repository.store_model(model=model, meta_props=model_props, training_data=df_train, training_target=df_model[\"FLAG_FOR_FRAUD_INV\"])\n    model_uid = wml_client.repository.get_model_uid(published_model_details)\n    print(\"Done\")",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "## Deploy the model\n\nIn this section, the model is deployed as a web service.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "wml_deployments = wml_client.deployments.get_details()\ndeployment_uid = None\nfor deployment in wml_deployments['resources']:\n    if DEPLOYMENT_NAME == deployment['entity']['name']:\n        deployment_uid = deployment['metadata']['guid']\n        break\n\nif deployment_uid is None:\n    print(\"Deploying model...\")\n\n    deployment = wml_client.deployments.create(artifact_uid=model_uid, name=DEPLOYMENT_NAME, asynchronous=False)\n    deployment_uid = wml_client.deployments.get_uid(deployment)\n    \nprint(\"Model id: {}\".format(model_uid))\nprint(\"Deployment id: {}\".format(deployment_uid))",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "The deployed model is available as a web service, and can be called via the scoring endpoint. Values are passed and predictions are returned as JSON objects.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "scoring_endpoint = None\n\nfor deployment in wml_client.deployments.get_details()['resources']:\n    if deployment_uid in deployment['metadata']['guid']:\n        scoring_endpoint = deployment['entity']['scoring_url']\n        \nprint(scoring_endpoint)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "fields = [\"SUSPICIOUS_CLAIM_TIME\", \"EXPIRED_LICENSE\", \"LOW_MILES_AT_LOSS\", \"EXCESSIVE_CLAIM_AMOUNT\", \"TOO_MANY_CLAIMS\", \"NO_POLICE\"]\nvalues = [[0,1,0,1,0,1]]\npayload_scoring = {\"fields\": fields,\"values\": values}\nscoring_response = wml_client.deployments.score(scoring_endpoint, payload_scoring)\nprint(scoring_response)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "## Configure OpenScale\n\nWe will now configure Watson OpenScale to monitor the deployed model. When this step is finished, all data into and out of the model will be logged, and can be made available to our applications via the Python API. Additionally, we will have the ability to generate explanations for individual predictions.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "from ibm_ai_openscale import APIClient\nfrom ibm_ai_openscale.engines import *\nfrom ibm_ai_openscale.utils import *\nfrom ibm_ai_openscale.supporting_classes import PayloadRecord, Feature\nfrom ibm_ai_openscale.supporting_classes.enums import *",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Get the unique identifier for the OpenScale instance.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "import requests\nfrom ibm_ai_openscale.utils import get_instance_guid\n\nWOS_GUID = get_instance_guid(api_key=CLOUD_API_KEY)\nWOS_CREDENTIALS = {\n    \"instance_guid\": WOS_GUID,\n    \"apikey\": CLOUD_API_KEY,\n    \"url\": \"https://api.aiopenscale.cloud.ibm.com\"\n}\n\nif WOS_GUID is None:\n    print('Watson OpenScale GUID NOT FOUND')\nelse:\n    print(WOS_GUID)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Create the OpenScale client.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "ai_client = APIClient(aios_credentials=WOS_CREDENTIALS)\nai_client.version",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "The code below creates the OpenScale datamart, a database in which OpenScale will store its data. If you have already set up OpenScale, it will use your existing datamart and not remove any previous data. If you specified Db2 Warehouse or Databases for PostgreSQL credentials above, it will use those credentials to create a datamart with that paid service. Finally, if you have not previously used OpenScale and did not supply credentials for a paid database service, it will create the datamart in a free, internal database. This internal database still allows access via the OpenScale APIs, but you cannot access it directly via database queries.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "try:\n    data_mart_details = ai_client.data_mart.get_details()\n    if 'internal_database' in data_mart_details and data_mart_details['internal_database']:\n        print('Using existing internal datamart.') \n    else:\n        print('Using existing external datamart')\nexcept:\n    if DB_CREDENTIALS is None:\n        print('Setting up internal datamart')\n        ai_client.data_mart.setup(internal_db=True)\n    else:\n        print('Setting up external datamart')\n        try:\n            ai_client.data_mart.setup(db_credentials=DB_CREDENTIALS)\n        except:\n            print('Setup failed, trying Db2 setup')\n            ai_client.data_mart.setup(db_credentials=DB_CREDENTIALS, schema=DB_CREDENTIALS['username'])",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Bind the OpenScale instance to the Watson Machine Learning instance. If you have already set up OpenScale, this cell will generate a warning that the binding already exists.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "binding_uid = ai_client.data_mart.bindings.add('WML instance', WatsonMachineLearningInstance(WML_CREDENTIALS))\nif binding_uid is None:\n    binding_uid = ai_client.data_mart.bindings.get_details()['service_bindings'][0]['metadata']['guid']\nbindings_details = ai_client.data_mart.bindings.get_details()\nai_client.data_mart.bindings.list()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "ai_client.data_mart.bindings.list_assets()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "The cells below will delete any existing OpenScale subscriptions for this particular model, ensuring that we have the most up-to-date model version. They will then create a new subscription for monitoring the new model.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\nfor subscription in subscriptions_uids:\n    sub_name = ai_client.data_mart.subscriptions.get_details(subscription)['entity']['asset']['name']\n    if sub_name == MODEL_NAME:\n        ai_client.data_mart.subscriptions.delete(subscription)\n        print('Deleted existing subscription for', MODEL_NAME)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "subscription = ai_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n    model_uid,\n    problem_type=ProblemType.BINARY_CLASSIFICATION,\n    input_data_type=InputDataType.STRUCTURED,\n    label_column='FLAG_FOR_FRAUD_INV',\n    prediction_column='prediction',\n    probability_column='probability',\n    feature_columns = [\"SUSPICIOUS_CLAIM_TIME\", \"EXPIRED_LICENSE\", \"LOW_MILES_AT_LOSS\", \"EXCESSIVE_CLAIM_AMOUNT\", \"TOO_MANY_CLAIMS\", \"NO_POLICE\"],\n    categorical_columns = []\n))",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "ai_client.data_mart.subscriptions.list()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Now that the datamart and subscription have been created, we need to send some sample data to the model for scoring so that OpenScale can create the correct schema for the payload logging table that will store our prediction history. These two records will be the two that we use for explanations as well.\n\nNote that we specify a transaction ID for the scoring request; this simulates a request coming from a user app, where the transaction ID matches the unique ID for the insurance claim, and will allow us to tie the prediction and explanation with a particular claim.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "fields = [\"SUSPICIOUS_CLAIM_TIME\", \"EXPIRED_LICENSE\", \"LOW_MILES_AT_LOSS\", \"EXCESSIVE_CLAIM_AMOUNT\", \"TOO_MANY_CLAIMS\", \"NO_POLICE\"]\nvalues = [[0,1,0,0,0,1]]\npayload_scoring = {\"fields\": fields,\"values\": values}\nscoring_response = wml_client.deployments.score(scoring_endpoint, payload_scoring, transaction_id='A2018MV533')\nprint(scoring_response)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "fields = [\"SUSPICIOUS_CLAIM_TIME\", \"EXPIRED_LICENSE\", \"LOW_MILES_AT_LOSS\", \"EXCESSIVE_CLAIM_AMOUNT\", \"TOO_MANY_CLAIMS\", \"NO_POLICE\"]\nvalues = [[0,0,0,1,1,0]]\npayload_scoring = {\"fields\": fields,\"values\": values}\nscoring_response = wml_client.deployments.score(scoring_endpoint, payload_scoring, transaction_id='A2016CA740')\nprint(scoring_response)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "Before we begin enabling OpenScale monitors, we pause for ten seconds to allow OpenScale time to create the schema in the prediction logging table in the datamart. We then enable quality monitoring, specifying an alert threshold of 0.7 and a minimum records threshold of 50. These settings mean that OpenScale will use a minimum of 50 records to calculate model quality, and alert us if the quality value falls below 70%.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "time.sleep(10)\nsubscription.quality_monitoring.enable(threshold=0.7, min_records=50)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "The next cell enables the explanation service in OpenScale, passing in the training data so that OpenScale can do some necessary calculations.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "from ibm_ai_openscale.supporting_classes import *\n\nsubscription.explainability.enable(training_data=df_model)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "With everything now configured, we can use the OpenScale Python client to import the contents of the payload logging table with the Pandas library.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "pandas_table_content = subscription.payload_logging.get_table_content()",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "pandas_table_content",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "The next two cells call the explanation service on our transactions, using the scoring IDs we provided. It should take between 30-60 seconds for each explanation to run. They can be run in background mode, but in this case we choose not to so the results can be displayed in the notebook.\n\nOnce the explanation service has evaluated a prediction, the data is saved in the OpenScale datamart and can be accessed without re-running the service.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "explain_run = subscription.explainability.run(transaction_id='A2018MV533-1', background_mode=False)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "pd.DataFrame.from_dict(explain_run['entity']['predictions'][0]['explanation_features'])",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "explain_run = subscription.explainability.run(transaction_id='A2016CA740-1', background_mode=False)",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "pd.DataFrame.from_dict(explain_run['entity']['predictions'][0]['explanation_features'])",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "source": "## Next Steps\n\nCongratulations, you have successfully run the notebook. Please return to the tutorial for instructions on setting up the Flask web application that accesses the data created here and makes it available to usuers.",
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": "",
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "version": "3.5.5",
   "name": "python",
   "file_extension": ".py",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "version": 3,
    "name": "ipython"
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4
}